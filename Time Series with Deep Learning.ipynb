{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series With Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will specifically look at:\n",
    "\n",
    "Univariate and multivariate Encoder-Decoder LSTMs for multi-step time series forecasting.\n",
    "\n",
    "CNN-LSTM Encoder-Decoder model for multi-step time series forecasting.\n",
    "\n",
    "ConvLSTM Encoder-Decoder model for multi-step time series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for pre-processing, cleaning up data and creating new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a function named fill_missing() that will take the NumPy array of the data and copy values from exactly 24 hours ago\n",
    "\n",
    "Then we can create a new column that contains the remainder of the sub-metering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean-up data\n",
    "from numpy import nan\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from pandas import to_numeric\n",
    "\n",
    "# fill missing values with a value at the same time one day ago\n",
    "def fill_missing(values):\n",
    "    one_day = 60 * 24\n",
    "    for row in range(values.shape[0]):\n",
    "        for col in range(values.shape[1]):\n",
    "            if np.isnan(values[row, col]):\n",
    "                values[row, col] = values[row - one_day, col]\n",
    "\n",
    "# load all data\n",
    "dataURL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip'\n",
    "#dataset = read_csv('household_power_consumption.txt', sep=';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])\n",
    "dataset = pd.read_csv(dataURL, sep=';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])\n",
    "# mark all missing values\n",
    "dataset.replace('?', nan, inplace=True)\n",
    "# make dataset numeric\n",
    "dataset = dataset.astype('float32')\n",
    "# fill missing\n",
    "fill_missing(dataset.values)\n",
    "# add a column for for the remainder of sub metering\n",
    "values = dataset.values\n",
    "dataset['sub_metering_4'] = (values[:,0] * 1000 / 60) - (values[:,4] + values[:,5] + values[:,6])\n",
    "# save updated dataset\n",
    "dataset.to_csv('household_power_consumption_days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting data and evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metric for this problem will be the RMSE for each lead time from day 1 to day 7.\n",
    "\n",
    "As a short-cut, it may be useful to summarize the performance of a model using a single score in order to aide in model selection.\n",
    "\n",
    "One possible score that could be used would be the RMSE across all forecast days.\n",
    "\n",
    "The function evaluate_forecasts() below will implement this behavior and return the performance of a model based on multiple seven-day forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into standard weeks\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into standard weeks\n",
    "    train, test = data[1:-328], data[-328:-6]\n",
    "    # restructure into windows of weekly data\n",
    "    train = array(split(train, len(train)/7))\n",
    "    test = array(split(test, len(test)/7))\n",
    "    return train, test\n",
    "\n",
    "# load the new file\n",
    "dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['Date'], index_col=['Date'])\n",
    "train, test = split_dataset(dataset.values)\n",
    "# validate train data\n",
    "print(train.shape)\n",
    "print(train[0, 0, 0], train[-1, -1, 0])\n",
    "# validate test\n",
    "print(test.shape)\n",
    "print(test[0, 0, 0], test[-1, -1, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk Forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models will be evaluated using a scheme called walk-forward validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "    # fit model\n",
    "    model = build_model(train, n_input)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_input)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "    history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Multi-Step Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model With Univariate Input and Vector Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into standard weeks\n",
    "    train, test = data[1:-328], data[-328:-6]\n",
    "    # restructure into windows of weekly data\n",
    "    train = array(split(train, len(train)/7))\n",
    "    test = array(split(test, len(test)/7))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "            x_input = x_input.reshape((len(x_input), 1))\n",
    "            X.append(x_input)\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 70, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "    # flatten data\n",
    "    data = array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, 0]\n",
    "    # reshape into [1, n_input, 1]\n",
    "    input_x = input_x.reshape((1, len(input_x), 1))\n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "    # fit model\n",
    "    model = build_model(train, n_input)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_input)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new file\n",
    "dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['Date'], index_col=['Date'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 7\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we improve the results?\n",
    "# evaluate model and get scores\n",
    "n_input = 14\n",
    "\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder LSTM Model With Univariate Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step encoder-decoder lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into standard weeks\n",
    "    train, test = data[1:-328], data[-328:-6]\n",
    "    # restructure into windows of weekly data\n",
    "    train = array(split(train, len(train)/7))\n",
    "    test = array(split(test, len(test)/7))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "            x_input = x_input.reshape((len(x_input), 1))\n",
    "            X.append(x_input)\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 20, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "    # flatten data\n",
    "    data = array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, 0]\n",
    "    # reshape into [1, n_input, 1]\n",
    "    input_x = input_x.reshape((1, len(input_x), 1))\n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "    # fit model\n",
    "    model = build_model(train, n_input)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_input)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new file\n",
    "dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['Date'], index_col=['Date'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 14\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder LSTM Model With Multivariate Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate multi-step encoder-decoder lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into standard weeks\n",
    "    train, test = data[1:-328], data[-328:-6]\n",
    "    # restructure into windows of weekly data\n",
    "    train = array(split(train, len(train)/7))\n",
    "    test = array(split(test, len(test)/7))\n",
    "    return train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "    \n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            X.append(data[in_start:in_end, :])\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return array(X), array(y)\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "    # flatten data\n",
    "    data = array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, :]\n",
    "    # reshape into [1, n_input, n]\n",
    "    input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 50, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(train, test, n_input):\n",
    "    # fit model\n",
    "    model = build_model(train, n_input)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_input)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new file\n",
    "dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['Date'], index_col=['Date'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 14\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation found that this model appears less stable than the univariate case and may be related to the differing scales of the input eight variables.\n",
    "\n",
    "We can see that in this case, the model is skillful, achieving an overall RMSE score of about 376 kilowatts.\n",
    "\n",
    "lstm: [376.273] 378.5, 381.5, 328.4, 388.3, 361.2, 308.0, 467.2 1 lstm: [376.273] 378.5, 381.5, 328.4, 388.3, 361.2, 308.0, 467.2 A line plot of the per-day RMSE is also created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM Encoder-Decoder Model With Univariate Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step encoder-decoder cnn-lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into standard weeks\n",
    "    train, test = data[1:-328], data[-328:-6]\n",
    "    # restructure into windows of weekly data\n",
    "    train = array(split(train, len(train)/7))\n",
    "    test = array(split(test, len(test)/7))\n",
    "    return train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "    \n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "            x_input = x_input.reshape((len(x_input), 1))\n",
    "            X.append(x_input)\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 20, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "    # flatten data\n",
    "    data = array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, 0]\n",
    "    # reshape into [1, n_input, 1]\n",
    "    input_x = input_x.reshape((1, len(input_x), 1))\n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "    # fit model\n",
    "    model = build_model(train, n_input)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_input)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new file\n",
    "dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['Date'], index_col=['Date'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 14\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little experimentation showed that using two convolutional layers made the model more stable than using just a single layer.\n",
    "\n",
    "We can see that in this case the model is skillful, achieving an overall RMSE score of about 372 kilowatts.\n",
    "\n",
    "lstm: [372.055] 383.8, 381.6, 339.1, 371.8, 371.8, 319.6, 427.2\n",
    "\n",
    "A line plot of the per-day RMSE is also created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM Encoder-Decoder Model With Univariate Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step encoder-decoder convlstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into standard weeks\n",
    "    train, test = data[1:-328], data[-328:-6]\n",
    "    # restructure into windows of weekly data\n",
    "    train = array(split(train, len(train)/7))\n",
    "    test = array(split(test, len(test)/7))\n",
    "    return train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "            x_input = x_input.reshape((len(x_input), 1))\n",
    "            X.append(x_input)\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_steps, n_length, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 20, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape into subsequences [samples, time steps, rows, cols, channels]\n",
    "    train_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_steps, n_length, n_input):\n",
    "    # flatten data\n",
    "    data = array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, 0]\n",
    "    # reshape into [samples, time steps, rows, cols, channels]\n",
    "    input_x = input_x.reshape((1, n_steps, 1, n_length, 1))\n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_steps, n_length, n_input):\n",
    "    # fit model\n",
    "    model = build_model(train, n_steps, n_length, n_input)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_steps, n_length, n_input)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new file\n",
    "dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['Date'], index_col=['Date'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# define the number of subsequences and the length of subsequences\n",
    "n_steps, n_length = 2, 7\n",
    "# define the total days to use as input\n",
    "n_input = n_length * n_steps\n",
    "score, scores = evaluate_model(train, test, n_steps, n_length, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
